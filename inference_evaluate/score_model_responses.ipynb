{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os, json, re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Grount Truths using prompts file\n",
    "def load_gts(model_responses_file):\n",
    "    gt_values_list_format = {}\n",
    "    ### open jsonl file\n",
    "    with open(model_responses_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            gt_value_unproccessed = data[\"gt\"]\n",
    "            ground_truth = re.sub(r\"(?<=\\d),(?=\\d)\", \"\", gt_value_unproccessed)\n",
    "            ground_truth = ground_truth.lower().strip()\n",
    "            ground_truth = [x.lower().strip() for x in ground_truth.replace(\"{\", \"\").replace(\"}\", \"\").replace(\"||\", \"|\").split(\"|\")]\n",
    "            \n",
    "            for j in range(len(ground_truth)): # Fix the commas in numbers problem\n",
    "                temp_val = ground_truth[j]\n",
    "                for _ in range(5):\n",
    "                    temp_val = re.sub(r'(\\d+),(\\d+)', r'\\1\\2', temp_val)\n",
    "                ground_truth[j] = temp_val\n",
    "\n",
    "            gt_values_list_format[data[\"qap_id\"]] = data[\"gt\"]\n",
    "    \n",
    "    return gt_values_list_format\n",
    "\n",
    "\n",
    "### Calculate MC Score - Mean Containment\n",
    "def calculate_containment_score(response, gt_tokens):\n",
    "    match_count = sum([1 for token in gt_tokens if token in response])\n",
    "    if len(gt_tokens) == 0:\n",
    "        return 0\n",
    "    return float(match_count / len(gt_tokens))\n",
    "\n",
    "\n",
    "### Clean model response if verbosse\n",
    "def post_process_response(response: str) -> str:\n",
    "    response = response.lower().strip()\n",
    "    # Define the regex pattern to match any of the junk tokens\n",
    "    junk_tokens = r\"(?:<eos_token>|#input|# input|# solution:|#solution|# explanation:|#explanation|note:|table:)\"\n",
    "    # Find the position of the first junk token\n",
    "    match = re.search(junk_tokens, response)\n",
    "    # If a junk token is found, truncate the response before it\n",
    "    if match:\n",
    "        response = response[:match.start()]\n",
    "    \n",
    "    ### Check for double new line (we see this trend quite often before models start yapping)\n",
    "    response = response.split(\"\\n\\n\")[0]\n",
    "    reponse = response.split(\"\\n```\\n\")[0]\n",
    "\n",
    "    # Remove '`' characters\n",
    "    response = response.replace(\"`\", \"\").replace(\"\\n\",\"\")\n",
    "\n",
    "    # Replace \",\" in numbers with \"\" (example: 1,000 -> 1000 & 1,232.23 -> 1232.23 & |1,000,000 -> |1000000)\n",
    "    for i in range(5):\n",
    "        response = re.sub(r'(\\d+),(\\d+)', r'\\1\\2', response)\n",
    "    \n",
    "    return response.lower().strip()\n",
    "\n",
    "### Main function for scoring\n",
    "def score_model_responses(model_responses_file, promtp_type = \"generic\"):\n",
    "    results = []\n",
    "    error_ids = []\n",
    "    try:\n",
    "        gts = load_gts(model_responses_file)\n",
    "    except:\n",
    "        gts = load_gts(\"/export/home/mshahmmer/hcsd_github_reformat/latest_inf_analysis/realWorld_HCT-all_prompts.jsonl\")\n",
    "\n",
    "\n",
    "    # Load model_responses_file as jsonl\n",
    "    with open(model_responses_file, 'r') as file:\n",
    "        all_responses = [json.loads(line) for line in file]\n",
    "\n",
    "    qap_ids_with_missing_gt = []\n",
    "\n",
    "    # Process each response\n",
    "    for record in all_responses:\n",
    "        qap_id = record[\"qap_id\"].split(\"&\")[0]\n",
    "        model_name = model_responses_file.split(\"--\")[1]\n",
    "        dataset_name = record[\"qap_id\"].split(\"--\")[0]\n",
    "\n",
    "        if qap_id in gts:\n",
    "            if record[\"response\"]  == None:\n",
    "                record[\"response\"] =  \"\"\n",
    "            raw_response = record[\"response\"].lower().strip()\n",
    "            cleaned_response = post_process_response(record[\"response\"]).lower().strip()\n",
    "\n",
    "            gt_t = [x.lower().strip() for x in gts[qap_id]]\n",
    "\n",
    "            raw_containment_score = calculate_containment_score(raw_response, gt_t)\n",
    "            raw_is_complete_containment = 1 if raw_containment_score == 1.0 else 0\n",
    "\n",
    "            cleaned_containment_score = calculate_containment_score(cleaned_response, gt_t)\n",
    "            cleaned_is_complete_containment = 1 if cleaned_containment_score == 1.0 else 0\n",
    "\n",
    "            results.append({\n",
    "                \"qap_id\": qap_id,\n",
    "                \"model_name\": model_name,\n",
    "                \"dataset_name\": dataset_name,\n",
    "                \"raw_response\": raw_response,\n",
    "                \"cleaned_response\": cleaned_response,\n",
    "                \"prompt_type\" : promtp_type,\n",
    "                \"gt\": gt_t,\n",
    "                \"MC_SCORE\": cleaned_containment_score,\n",
    "                \"CC_SCORE\": cleaned_is_complete_containment,\n",
    "                \"raw_response_length\": len(raw_response.split()),\n",
    "                \"cleaned_response_length\": len(cleaned_response.split()),\n",
    "                # decrease in token count after cleaning\n",
    "                \"perc_junk_tokens_in_raw_response\": ((len(raw_response) - len(cleaned_response))*100) / (len(raw_response)+1)\n",
    "            })\n",
    "        else:\n",
    "            qap_ids_with_missing_gt.append(record[\"qap_id\"])\n",
    "\n",
    "    return pd.DataFrame(results), qap_ids_with_missing_gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET USER INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Give path to where all model responses that are to be scored are saved\n",
    "model_responses_folder = \"../results/model_responses/\"\n",
    "\n",
    "### Give path to where all scores files are to be saved\n",
    "scores_folder = \"../results/scores/\"\n",
    "# scores_folder = \"\"\n",
    "\n",
    "### *OPTIONAL* Prompt type if specfic one is to provided\n",
    "prompt_type = \"generic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results--Meta-Llama-3.1-8B-Instruct--realWorld_HCT-all_promptsl.jsonl\n",
      "Model: results - Missing GTs: 0\n",
      "Model: results - Results DF Shape: (9835, 12)\n"
     ]
    }
   ],
   "source": [
    "### GIT TEST\n",
    "model_name_to_results_df = {}\n",
    "for fname in os.listdir(model_responses_folder):\n",
    "    print(fname)\n",
    "    model_name_t = fname.split(\"--\")[0]    \n",
    "    model_response_file_t = os.path.join(model_responses_folder, fname)\n",
    "    results_df_t, missing_gt_ids = score_model_responses(model_response_file_t)\n",
    "    print(f\"Model: {model_name_t} - Missing GTs: {len(missing_gt_ids)}\")\n",
    "    print(f\"Model: {model_name_t} - Results DF Shape: {results_df_t.shape}\")\n",
    "    model_name_to_results_df[model_name_t] = results_df_t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9835, 12)\n"
     ]
    }
   ],
   "source": [
    "all_results_df = pd.concat([v for k,v in model_name_to_results_df.items()])\n",
    "all_results_df = all_results_df.reset_index(drop=True)\n",
    "print(all_results_df.shape)\n",
    "\n",
    "### Collapse on model_name per dataset_name level\n",
    "all_results_df_grouped_by_model_dataset = all_results_df.groupby([\"model_name\", \"dataset_name\"]).agg({\n",
    "    \"MC_SCORE\": \"mean\",\n",
    "    \"CC_SCORE\": \"mean\",\n",
    "    \"raw_response_length\": \"mean\",\n",
    "    \"cleaned_response_length\": \"mean\",\n",
    "    \"perc_junk_tokens_in_raw_response\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "all_results_df_grouped_by_model_dataset.to_csv(os.path.join(scores_folder, f\"{model_responses_folder.split('/')[-1]}--perModelDataset-scores.csv\"), index=False) \n",
    "\n",
    "### Collapse on dataset_name\n",
    "all_results_df_grouped_by_model = all_results_df.groupby([\"model_name\"]).agg({\n",
    "    \"MC_SCORE\": \"mean\",\n",
    "    \"CC_SCORE\": \"mean\",\n",
    "    \"raw_response_length\": \"mean\",\n",
    "    \"cleaned_response_length\": \"mean\",\n",
    "    \"perc_junk_tokens_in_raw_response\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "all_results_df_grouped_by_model.to_csv(os.path.join(scores_folder, f\"{model_responses_folder.split('/')[-1]}--perModel-scores.csv\"), index=False) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additonal Collapsed Results for Format Variation Scoring ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Collapse on Model - Dataset - Format Type\n",
    "all_results_df_grouped_by_model_dataset_prompt = all_results_df.groupby([\"model_name\", \"dataset_name\", \"prompt_type\"]).agg({\n",
    "    \"MC_SCORE\": \"mean\",\n",
    "    \"CC_SCORE\": \"mean\",\n",
    "    \"raw_response_length\": \"mean\",\n",
    "    \"cleaned_response_length\": \"mean\",\n",
    "    \"perc_junk_tokens_in_raw_response\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "all_results_df_grouped_by_model_dataset_prompt.to_csv(os.path.join(scores_folder, f\"{model_responses_folder.split('/')[-1]}--perModelDatasetFormat--formatVariation-scores.csv\"), index=False) \n",
    "\n",
    "### Collapse on Model - Format Type\n",
    "all_results_df_grouped_by_model_prompt = all_results_df.groupby([\"model_name\", \"prompt_type\"]).agg({\n",
    "    \"MC_SCORE\": \"mean\",\n",
    "    \"CC_SCORE\": \"mean\",\n",
    "    \"raw_response_length\": \"mean\",\n",
    "    \"cleaned_response_length\": \"mean\",\n",
    "    \"perc_junk_tokens_in_raw_response\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "all_results_df_grouped_by_model_prompt.to_csv(os.path.join(scores_folder, f\"{model_responses_folder.split('/')[-1]}--perModelFormat--formatVariation-scores.csv\"), index=False) \n",
    "\n",
    "### Collapse on Format Type\n",
    "all_results_df_grouped_by_prompt = all_results_df.groupby([\"prompt_type\"]).agg({\n",
    "    \"MC_SCORE\": \"mean\",\n",
    "    \"CC_SCORE\": \"mean\",\n",
    "    \"raw_response_length\": \"mean\",\n",
    "    \"cleaned_response_length\": \"mean\",\n",
    "    \"perc_junk_tokens_in_raw_response\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "all_results_df_grouped_by_model.to_csv(os.path.join(scores_folder, f\"{model_responses_folder.split('/')[-1]}--perFormat--formatVariation-scores.csv\"), index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ojv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
